# Simulation type. Possible values:
# "pinocchio": Pinocchio mocks varying only Omega_m and sigma8.
# "pinocchio_lcdm": Pinocchio mocks varying all flat LCDM parameters (wo neutrinos): Omega_m, sigma8, ns, h, Omega_b
# "abacus": Abacus simulations (not fully supported for training the network).
sim_type = "pinocchio_lcdm"

# Names of the cosmological parameters used for inference.
cosmo_params_names = ["Omega_m", "sigma8", "h", "n_s", "Omega_b"]
# File with the values of the cosmological parameters for each simulation.
cosmo_params_file = "/exa/projects/MLS/inigo.saez/models_parameters/models_parameters_sobol_lcdm.txt"

# Number of X-ray illumination models to use.
# If equal to zero, use only the fiducial model.
# If different from zero, use models from the sobol sequence.
# Only used if mobs_type="xlum" for at least one of the considered probes.
xlum_sobol_n_models = 0
# File with the values of the X-ray illumination parameters.
xlum_params_file = "/exa/projects/MLS/inigo.saez/models_parameters/sobol_samples_Nmodels_8samples_6sigma.npz"

# Fraction of the total dataset to be used.
fraction_total = 1
# Validation and test datasets fractions (applied after fraction_total).
fraction_validation = 0.1
fraction_test = 0.1
# Seed used to split datasets and select fractions.
split_seed = 1234

# Verbose output.
verbose = true

# Lazy data loading.
# If False, read whole dataset from disk at once into memery and send everything to device (CPU/GPU).
# If True, read data from disk and send to device by batches.
lazy_loading = false

[train]
# These are the parameters used to train the network.

# If True, compile the model. See https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html
# This option is only valid for the training phase, and not the HP tuning.
# See below for the HP tuning phase.
compile_model = false
# Compile mode. See https://docs.pytorch.org/docs/stable/generated/torch.compile.html
compile_mode = "default"

# Maximum number of epochs.
n_epochs = 1000
# Patience for early stopping.
patience_early_stopping = 10
# Early stopping patience can also be defined as a constant factor with respect to reduce_on_plateau_patience.
# i.e. patience = int(patience_early_stopping_factor x reduce_on_plateau_patience)
# If different from zero, this definition is used.
patience_early_stopping_factor = 2.5

# Settings for the loss function.

# Include a constraint on the skewness in the loss.
# Ignored if gauss_nllloss=true.
loss_skew = false
# Include a constraint on the kurtosis in the loss. WARNING: not fully implemented. Needs to be checked.
# Ignored if gauss_nllloss=true.
loss_kurt = false
# Use Gaussian negative log likelihood loss from pytorch (GaussianNLLLoss).
gauss_nllloss = false

# Use training parameters from previous tuning run.
# If true, the values of the remaining tranining parameters of this section will not be used.
train_from_tune = true
# Directory with the results of the tuning run. If set to "output_dir" use the same path as output_dir field.
tune_dir = "output_dir"
# Name of the optuna study to be used.
study_name = "mystudy"

# Patience for ReduceOnPlateau learning rate scheduler.
reduce_on_plateau_patience = 10
reduce_on_plateau_factor = 0.1

# Batch size for the training dataset.
batch_size = 32

# Initial learning rate for Adam" optimizer.
learning_rate = 1e-3
# Weight decay for AdamW optimizer.
weight_decay = 0.01
# Beta values for AdamW optimizer.
beta_1 = 0.9
beta_2 = 0.999
# Epsilon value for AdamW optimizer.
eps = 1e-8

# Dropout rate.
dropout = 0

# If true, use BatchNormalization.
batch_norm = true

# Parameters for the final regression FC block.
regressor_fc_layers = 2
regressor_fc_units_per_layer = 128

# Parameters for the power spectrum FC block.
ps_fc_layers = 2
ps_fc_units_per_layer = 128

# Parameters for the number counts FC block.
nc_fc_layers = 2
nc_fc_units_per_layer = 128

# Parameters for the density_field convolutional block.
density_field_n_channels_first = 10

[probes]
# These parameters control the cosmological probes that are used for inference.

# List of probes to be used.
# Parameters for probes that are not in this list are ignored.
probe_list = ["density_field"]

# Root directory relative to which the data directories for each probes are defined.
data_dir_root = "/exa/projects/MLS/inigo.saez/pinocchio_observables/L1500_N750_sobol_lcdm"

[probes.density_field]
# Different mass/luminosity thresholds and redshifts are combined as different channels.
# The total number of channels is n_mobs * n_z, where n_mobs is the number of mass/luminosity thresholds and n_z the
# number of redshifts.

# Directory with the data relative to data_dir_root. Different redshifts and mass/luminosity selections are in sub-directories.
data_dir = "density_field_n32_norsd/"
# List of minimum mass/luminosity thresholds.
mobs_min = [3.6e13, 6e13, 1e14, 1.7e14, 2.8e14, 4.6e14, 7.7e14]
# Use mass or luminosity cuts.
# mass: "mass"
# luminosity: "xlum"
mobs_type = "mass"
# List of redshifts (snapshots).
redshift = [0.2]

n_augment_flip = 7

[probes.power_spectrum]
# Power spectrum from different mass thresholds and redshifts are concatenated into a single 1D array.

# Root directory with the data. Different mass cuts and redshifts are in sub-directories.
data_dir = "power_spectrum_n128_norsd"
# List of minimum mass/luminosity thresholds.
mobs_min = [3.6e13, 6e13, 1e14, 1.7e14, 2.8e14, 4.6e14, 7.7e14]
# Use mass or luminosity cuts.
# mass: "mass"
# luminosity: "xlum"
mobs_type = "mass"
# List of redshifts (snapshots).
redshift = [0.2]

[probes.number_counts]
# Number counts from different redshifts are concatenated into a single 1D array.

# Directory with the data relative to data_dir_root. Different redshifts are in sub-directories.
data_dir = "number_counts"
# List of minimum mass/luminosity thresholds.
mobs_min = [3.6e13, 6e13, 1e14, 1.7e14, 2.8e14, 4.6e14, 7.7e14]
# Use mass or luminosity cuts.
# mass: "mass"
# luminosity: "xlum"
mobs_type = "mass"
# List of redshifts (snapshots).
redshift = [0.2]
# If true, use cumulative mass bins. If false, use differential mass bins. WARNING: not yet fully implemented.
cumulative = true

[tune]
# These are the parameters used to tune the hyperparameters of the network.

# Name of the optuna study.
study_name = "mystudy"

# Resume from previous study with same name if it exists.
resume = true

# If True, compile the model. See https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html
# This option is only valid for the tuning phase, and not the training phase.
# See above for the training phase.
compile_model = false
# Compile mode. See https://docs.pytorch.org/docs/stable/generated/torch.compile.html
compile_mode = "default"

# Number of trials per process.
trials = 400
# Maximum number of epochs for each trial.
n_epochs = 500

# Pruner to be used. Possible values: median, hyperband (see optuna documentation).
pruner = "median"

# Parameters for median pruner (see optuna documentation).
median_n_startup_trials = 5
median_n_warmup_steps = 10
median_n_min_trials = 1

# Parameters for hyperband pruner (see optuna documentation).
hyperband_min_resource = 10
hyperband_reduction_factor = 3

[tune.learning_rate]

low = 1e-4
high = 1e-1
log = true

[tune.weight_decay]

low = 1e-7
high = 1e-1
log = true

[tune.beta_1]

low = 0.85
high = 0.999

[tune.beta_2]

low = 0.99
high = 0.9999

[tune.eps]

low = 1e-9
high = 1e-6
log = true

[tune.batch_size]

choices = [16, 32, 64, 128, 256, 512, 1024]

[tune.regressor_fc_layers]

low = 1
high = 10
step = 1

[tune.regressor_fc_units_per_layer]

low = 100
high = 1000
step = 100

[tune.ps_fc_layers]

low = 1
high = 10
step = 1

[tune.ps_fc_units_per_layer]

low = 100
high = 1000
step = 100

[tune.nc_fc_layers]

low = 1
high = 10
step = 1

[tune.nc_fc_units_per_layer]

low = 100
high = 1000
step = 100

[tune.density_field_n_channels_first]

low = 5
high = 100
step = 5

[tune.dropout]

low = 0
high = 0.5

[tune.reduce_on_plateau_factor]

low = 0.01
high = 0.9
log = true
